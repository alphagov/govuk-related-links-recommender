#!/bin/bash

set -eo pipefail
source ~/.profile

function file_or_die {
    if [ -s $1 ]; then
        echo "Found $1. All good"
    else
        echo "$1 not found or is empty. Exiting"
        exit 255
    fi
}

echo "Starting run_link_generation script..."
sleep 60

echo "Running link generation in background..."
echo "Writing log to /var/tmp/related_links_process.log"

export DATA_DIR=$PWD/data
mkdir -p $DATA_DIR # make sure it exists
export GOOGLE_APPLICATION_CREDENTIALS=/var/tmp/bigquery.json

###########################################################################################
# Python setup
###########################################################################################

echo "### Installing python packages"
cd /var/data/github/govuk-related-links-recommender
echo "Installing setuptools first"
python3.6 -m pip install --upgrade setuptools
echo "Installing local requirements"
python3.6 -m pip install -r requirements-original.txt

echo "Python version installed:"
python3.6 -V

###########################################################################################
# Create a local copy of the content store
###########################################################################################

# Find and download the latest content store backup from S3
echo "Finding latest content backup..."
LATEST_CONTENT_BACKUP_PATH=$(aws s3api list-objects-v2 --bucket $CONTENT_STORE_BUCKET --prefix mongo-api --query "Contents[?contains(Key, '-content_store_production.gz')]" | jq  -c "max_by(.LastModified)|.Key" | xargs)

echo "Downloading latest content store backup..."
aws s3 cp s3://$CONTENT_STORE_BUCKET/$LATEST_CONTENT_BACKUP_PATH /var/data/latest_content_store_backup.gz --no-progress

# Extract content store backup
cd /var/data
tar -xvf latest_content_store_backup.gz
ls -lat content_store_production

# Restore content store data to MongoDb
mongorestore -d content_store -c content_items /var/data/content_store_production/content_items.bson

###########################################################################################
# Start related-links generation
###########################################################################################

cd /var/data/github/govuk-related-links-recommender

echo -e "\n\n### Starting related links generation process\n"

echo -e "\n\n# Generating structural edges\n"
# Input: mongodb
python3.6 src/data_preprocessing/get_content_store_data.py
file_or_die "$DATA_DIR/structural_edges.csv"
file_or_die "$DATA_DIR/content_id_base_path_mapping.json"
file_or_die "$DATA_DIR/page_path_content_id_mapping.json"
file_or_die "$DATA_DIR/eligible_source_content_ids.pkl"
file_or_die "$DATA_DIR/eligible_target_content_ids.pkl"
aws s3 cp $DATA_DIR/structural_edges.csv s3://$RELATED_LINKS_BUCKET/structural_edges.csv --no-progress
aws s3 cp $DATA_DIR/content_id_base_path_mapping.json s3://$RELATED_LINKS_BUCKET/content_id_base_path_mapping.json --no-progress
aws s3 cp $DATA_DIR/page_path_content_id_mapping.json s3://$RELATED_LINKS_BUCKET/page_path_content_id_mapping.json --no-progress
aws s3 cp $DATA_DIR/eligible_source_content_ids.pkl s3://$RELATED_LINKS_BUCKET/eligible_source_content_ids.pkl --no-progress
aws s3 cp $DATA_DIR/eligible_target_content_ids.pkl s3://$RELATED_LINKS_BUCKET/eligible_target_content_ids.pkl --no-progress

echo -e "\n\n# Generating functional edges\n"
# Input: Big query
python3.6 src/data_preprocessing/make_functional_edges_and_weights.py
file_or_die "$DATA_DIR/functional_edges.csv"
aws s3 cp $DATA_DIR/functional_edges.csv s3://$RELATED_LINKS_BUCKET/functional_edges.csv --no-progress

echo -e "\n\n# Running make_weighted_network\n"
# Input:
#  - structural_edges.csv
#  - functional_edges.csv
# Output: network.csv
python3.6 src/features/make_weighted_network.py
file_or_die "$DATA_DIR/network.csv"
aws s3 cp $DATA_DIR/network.csv s3://$RELATED_LINKS_BUCKET/network.csv --no-progress

echo -e "\n\n# Running train_node2vec_model\n"
# Input: network.csv
python3.6 src/models/train_node2vec_model.py
file_or_die "$DATA_DIR/n2v.model"
file_or_die "$DATA_DIR/n2v_node_embeddings"
# NOTE: Word2Vec.save() can generate more files, with unspecified names. See
# https://stackoverflow.com/questions/47173538/why-are-multiple-model-files-created-in-gensim-word2vec
aws s3 cp $DATA_DIR/n2v.model s3://$RELATED_LINKS_BUCKET/n2v.model --no-progress
aws s3 cp $DATA_DIR/n2v_node_embeddings s3://$RELATED_LINKS_BUCKET/n2v_node_embeddings --no-progress


echo -e "\n\n# Running predicted_related_links\n"
# Input:
#  - Big Query
#  - n2v.model
#  - content_id_base_path_mapping.json
#  - eligible_source_content_ids.pkl
#  - eligible_target_content_ids.pkl
python3.6 src/models/predict_related_links.py
file_or_die "${DATA_DIR}/suggested_related_links.json"
file_or_die "${DATA_DIR}/suggested_related_links.tsv"
TIMESTAMP=`date +%Y%m%d`
aws s3 cp ${DATA_DIR}/suggested_related_links.json s3://$RELATED_LINKS_BUCKET/${TIMESTAMP}suggested_related_links.json --no-progress
aws s3 cp ${DATA_DIR}/suggested_related_links.tsv s3://$RELATED_LINKS_BUCKET/${TIMESTAMP}suggested_related_links.tsv --no-progress

echo -e "\n\n# List out all generated data files\n"
ls -lR $DATA_DIR

echo -e "\n\n# Copying logs to S3\n"
aws s3 cp /tmp/govuk-related-links-recommender.log s3://$RELATED_LINKS_BUCKET/govuk-related-links-recommender.log --no-progress
aws s3 cp /var/tmp/related_links_process.log s3://$RELATED_LINKS_BUCKET/related_links_generation.log --no-progress

echo "related_links process succeeded"
