{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environment vars and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_DIR = os.path.join(\"/Users/felisialoukou/Documents/\",\"govuk-network-data\", \"key\")\n",
    "KEY_PATH = os.path.join(KEY_DIR, os.listdir(KEY_DIR)[0])\n",
    "PROJECT_ID = \"govuk-bigquery-analytics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging for `pandas_gbq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('pandas_gbq')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Extract page-hit only user journeys for 2-3 weeks back\n",
    "8.8 GB\n",
    "### Define date range\n",
    "    > 3 wks before, up to current date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-18 12:07:19.967568\n",
      "20190327 20190417\n"
     ]
    }
   ],
   "source": [
    "N_weeks = 3\n",
    "yesterday = 1\n",
    "N_days = (N_weeks*7) + yesterday\n",
    "\n",
    "start = (datetime.now() - timedelta(days=N_days)).date().strftime(\"%Y%m%d\")\n",
    "end = (datetime.now() - timedelta(days=yesterday)).date().strftime(\"%Y%m%d\")\n",
    "\n",
    "print(datetime.now())\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_20190327_20190327_'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daterange = \"_{}_{}_\".format(start, end)\n",
    "daterange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT\n",
    "  COUNT(*) AS Occurrences,\n",
    "  PageSeq_Length,\n",
    "  PageSequence,\n",
    "  CIDSequence\n",
    "FROM (\n",
    "  SELECT\n",
    "    *\n",
    "  FROM (\n",
    "    SELECT\n",
    "      CONCAT(fullVisitorId,\"-\",CAST(visitId AS STRING),\"-\",CAST(visitNumber AS STRING)) AS sessionId,\n",
    "      STRING_AGG(IF(htype = 'PAGE',\n",
    "          pagePath,\n",
    "          NULL),\">>\") OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS PageSequence,\n",
    "       STRING_AGG(IF(htype = 'PAGE',\n",
    "          content_id,\n",
    "          NULL),\">>\") OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS CIDSequence,\n",
    "      SUM(IF(htype='PAGE',\n",
    "          1,\n",
    "          0)) OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS PageSeq_Length\n",
    "    FROM (\n",
    "      SELECT\n",
    "        fullVisitorId,\n",
    "        visitId,\n",
    "        visitNumber,\n",
    "        visitStartTime,\n",
    "        hits.page.pagePath AS pagePath,\n",
    "        hits.hitNumber AS hitNumber,\n",
    "        hits.type AS htype,\n",
    "        (\n",
    "        SELECT\n",
    "          value\n",
    "        FROM\n",
    "          hits.customDimensions\n",
    "        WHERE\n",
    "          index=4) AS content_id,\n",
    "        (\n",
    "        SELECT\n",
    "          value\n",
    "        FROM\n",
    "          hits.customDimensions\n",
    "        WHERE\n",
    "          index=2) AS document_type\n",
    "      FROM\n",
    "        `govuk-bigquery-analytics.87773428.ga_sessions_*` AS sessions\n",
    "      CROSS JOIN\n",
    "        UNNEST(sessions.hits) AS hits\n",
    "      WHERE\n",
    "        _TABLE_SUFFIX BETWEEN '_start_'\n",
    "        AND '_end_' )\n",
    "    WHERE\n",
    "      pagePath != '/'\n",
    "      AND document_type NOT IN (\n",
    "        'document_collection',\n",
    "        'finder',\n",
    "        'homepage',\n",
    "        'license_finder',\n",
    "        'mainstream_browse_page',\n",
    "        'organisation',\n",
    "        'search',\n",
    "        'service_manual_homepage',\n",
    "        'service_manual_topic',\n",
    "        'services_and_information',\n",
    "        'taxon',\n",
    "        'topic',\n",
    "        'topical_event',\n",
    "        'fatality_notice',\n",
    "        'contact',\n",
    "        'service_sign_in',\n",
    "        'html_publication',\n",
    "        'calculator',\n",
    "        'completed_transaction' )\n",
    "      AND content_id NOT IN ( '00000000-0000-0000-0000-000000000000',\n",
    "        '[object Object]'))\n",
    "  WHERE\n",
    "    PageSeq_Length >1\n",
    "  GROUP BY\n",
    "    sessionId,\n",
    "    PageSequence,\n",
    "    CIDSequence,\n",
    "    PageSeq_Length)\n",
    "GROUP BY\n",
    "  PageSequence,\n",
    "  CIDSequence,\n",
    "  PageSeq_Length\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT\\n  COUNT(*) AS Occurrences,\\n  PageSeq_Length,\\n  PageSequence,\\n  CIDSequence\\nFROM (\\n  SELECT\\n    *\\n  FROM (\\n    SELECT\\n      CONCAT(fullVisitorId,\"-\",CAST(visitId AS STRING),\"-\",CAST(visitNumber AS STRING)) AS sessionId,\\n      STRING_AGG(IF(htype = \\'PAGE\\',\\n          pagePath,\\n          NULL),\">>\") OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS PageSequence,\\n       STRING_AGG(IF(htype = \\'PAGE\\',\\n          content_id,\\n          NULL),\">>\") OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS CIDSequence,\\n      SUM(IF(htype=\\'PAGE\\',\\n          1,\\n          0)) OVER (PARTITION BY fullVisitorId, visitId, visitStartTime ORDER BY hitNumber ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING ) AS PageSeq_Length\\n    FROM (\\n      SELECT\\n        fullVisitorId,\\n        visitId,\\n        visitNumber,\\n        visitStartTime,\\n        hits.page.pagePath AS pagePath,\\n        hits.hitNumber AS hitNumber,\\n        hits.type AS htype,\\n        (\\n        SELECT\\n          value\\n        FROM\\n          hits.customDimensions\\n        WHERE\\n          index=4) AS content_id,\\n        (\\n        SELECT\\n          value\\n        FROM\\n          hits.customDimensions\\n        WHERE\\n          index=2) AS document_type\\n      FROM\\n        `govuk-bigquery-analytics.87773428.ga_sessions_*` AS sessions\\n      CROSS JOIN\\n        UNNEST(sessions.hits) AS hits\\n      WHERE\\n        _TABLE_SUFFIX BETWEEN \\'20190327\\'\\n        AND \\'20190327\\' )\\n    WHERE\\n      pagePath != \\'/\\'\\n      AND document_type NOT IN (\\n        \\'document_collection\\',\\n        \\'finder\\',\\n        \\'homepage\\',\\n        \\'license_finder\\',\\n        \\'mainstream_browse_page\\',\\n        \\'organisation\\',\\n        \\'search\\',\\n        \\'service_manual_homepage\\',\\n        \\'service_manual_topic\\',\\n        \\'services_and_information\\',\\n        \\'taxon\\',\\n        \\'topic\\',\\n        \\'topical_event\\',\\n        \\'fatality_notice\\',\\n        \\'contact\\',\\n        \\'service_sign_in\\',\\n        \\'html_publication\\',\\n        \\'calculator\\',\\n        \\'completed_transaction\\' )\\n      AND content_id NOT IN ( \\'00000000-0000-0000-0000-000000000000\\',\\n        \\'[object Object]\\'))\\n  WHERE\\n    PageSeq_Length >1\\n  GROUP BY\\n    sessionId,\\n    PageSequence,\\n    CIDSequence,\\n    PageSeq_Length)\\nGROUP BY\\n  PageSequence,\\n  CIDSequence,\\n  PageSeq_Length'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = query.replace(\"_start_\", start).replace(\"_end_\", end)\n",
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract data from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... \n",
      "Query running...\n",
      "Job ID: f875a606-d968-4667-99a2-8115efae7808\n",
      "  Elapsed 6.44 s. Waiting...\n",
      "  Elapsed 7.87 s. Waiting...\n",
      "  Elapsed 9.31 s. Waiting...\n",
      "  Elapsed 10.74 s. Waiting...\n",
      "  Elapsed 12.01 s. Waiting...\n",
      "  Elapsed 13.29 s. Waiting...\n",
      "  Elapsed 14.73 s. Waiting...\n",
      "  Elapsed 16.17 s. Waiting...\n",
      "  Elapsed 17.6 s. Waiting...\n",
      "  Elapsed 19.03 s. Waiting...\n",
      "  Elapsed 20.47 s. Waiting...\n",
      "  Elapsed 21.81 s. Waiting...\n",
      "  Elapsed 23.23 s. Waiting...\n",
      "  Elapsed 24.67 s. Waiting...\n",
      "Query done.\n",
      "Processed: 1.5 GB Billed: 1.5 GB\n",
      "Standard price: $0.01 USD\n",
      "\n",
      "Got 1069433 rows.\n",
      "\n",
      "Total time taken 138.99 s.\n",
      "Finished at 2019-04-18 12:25:30.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1069433, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_in = pd.read_gbq(query,\n",
    "                       project_id=PROJECT_ID,\n",
    "                       reauth=False,\n",
    "                       private_key=KEY_PATH,\n",
    "                       dialect=\"standard\")\n",
    "\n",
    "df_in.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add `Page_List` and `Cid_List` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagelist = [pageseq.split(\">>\") for pageseq in df_in['PageSequence'].values]\n",
    "df_in['Page_List'] = pagelist\n",
    "\n",
    "cidlist = [cidseq.split(\">>\") for cidseq in df_in['CIDSequence'].values]\n",
    "df_in['Cid_List'] = cidlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dir = os.path.join(os.getenv(\"DATA_DIR\"),\"raw\", \"bq_journey_extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drop infrequent journeys\n",
    "bq_file_doo = os.path.join(bq_dir, \"page_cid_seq_user_journey\"+date_range+\"doo.csv.gz\")\n",
    "df_in[df_in.Occurrences>1].to_csv(bq_file_doo, compression=\"gzip\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, columns_to_read):\n",
    "    \"\"\"\n",
    "    Read a dataframe compressed csv file, init as dataframe, drop unnecessary columns, prepare target columns\n",
    "    to be evaluated as lists with literal_eval.\n",
    "    :return: processed for list-eval dataframe\n",
    "    \"\"\"\n",
    "    logger.debug(\"Reading file {}...\".format(filename))\n",
    "    df = pd.read_csv(filename, sep='\\t', compression=\"gzip\", skipinitialspace=True, usecols=columns_to_read)\n",
    "    logger.debug(\"Read in {} columns...\".format(df.columns))\n",
    "\n",
    "#     print(df.shape)\n",
    "#     print(df[df.Occurrences == 1].shape)\n",
    "#     # Sample 30% of one-off journeys and then use these indices to drop them\n",
    "#     indices = df[df.Occurrences == 1].sample(frac=0.3, random_state=1234).index\n",
    "#     print(len(indices))\n",
    "#     df.drop(indices, inplace=True)\n",
    "#     print(df.shape)\n",
    "\n",
    "#     logger.debug(\"Number of rows post one-off occurrence drop: {}\".format(df.shape))\n",
    "\n",
    "    column_to_eval = 'Cid_List'\n",
    "\n",
    "\n",
    "    if isinstance(df[column_to_eval].iloc[0], str) and any([\",\" in val for val in df[column_to_eval].values]):\n",
    "        logger.debug(\"Working on literal_eval for \\\"{}\\\"\".format(column_to_eval))\n",
    "        df[column_to_eval] = df[column_to_eval].map(literal_eval)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_occurrences(user_journey_df, page_sequence, occurrences):\n",
    "    logging.debug(\"Computing specialized occurrences \\\"{}\\\" based on  \\\"{}\\\"...\".format(occurrences, page_sequence))\n",
    "    user_journey_df[occurrences] = user_journey_df.groupby(page_sequence)['Occurrences'].transform(\n",
    "        'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subpaths(user_journey_df, page_list, subpaths):\n",
    "    \"\"\"\n",
    "    Compute lists of subpaths ie node-pairs/edges (where a node is a page) from both original and de-looped page_lists\n",
    "    (page-hit only journeys)\n",
    "    :param subpaths:\n",
    "    :param page_list:\n",
    "    :param user_journey_df: user journey dataframe\n",
    "    :return: inplace assign new columns\n",
    "    \"\"\"\n",
    "    logger.debug(\"Setting up \\\"{}\\\" based on  \\\"{}\\\"...\".format(subpaths, page_list))\n",
    "    user_journey_df[subpaths] = user_journey_df[page_list].map(prep.subpaths_from_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgelist_from_subpaths(user_journey_df, use_delooped_journeys=False):\n",
    "    \"\"\"\n",
    "    Generate a counter that represents the edge list. Keys are edges (node pairs) which represent a user going from\n",
    "    first element of pair to second one), values are a sum of journey occurrences (de-looped occurrences since current\n",
    "    computation is based on de-looped subpaths), ie number of times a user/agent went from one page (node) to another.\n",
    "    :param use_delooped_journeys:\n",
    "    :param user_journey_df: user journey dataframe\n",
    "    :return: edgelist counter\n",
    "    \"\"\"\n",
    "    subpath_default = 'Cid_Subpaths'\n",
    "    occurrences_default = 'CIDSeq_Occurrences'\n",
    "    page_list_default = 'Cid_List'\n",
    "    page_sequence_default = 'CIDSequence'\n",
    "\n",
    "    if occurrences_default not in user_journey_df.columns:\n",
    "        compute_occurrences(user_journey_df, page_sequence_default, occurrences_default)\n",
    "\n",
    "    logger.debug(\"Dropping duplicates {}...\".format(page_sequence_default))\n",
    "    user_journey_df.drop_duplicates(page_sequence_default, keep=\"first\", inplace=True)\n",
    "\n",
    "    generate_subpaths(user_journey_df, page_list_default, subpath_default)\n",
    "    edgelist_counter = Counter()\n",
    "\n",
    "    ind_path = user_journey_df.columns.get_loc(subpath_default)\n",
    "    ind_occ = user_journey_df.columns.get_loc(occurrences_default)\n",
    "\n",
    "    for tup in user_journey_df.itertuples(index=False):\n",
    "        for edge in tup[ind_path]:\n",
    "            edgelist_counter[tuple(edge)] += tup[ind_occ]\n",
    "\n",
    "    return edgelist_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_from_edgelist(edgelist):\n",
    "    \"\"\"\n",
    "    Generate a node list (from edges). Internally represented as a set, returned as alphabetically sorted list\n",
    "    :param edgelist: list of edges (node-pairs)\n",
    "    :return: sorted list of nodes\n",
    "    \"\"\"\n",
    "    logger.debug(\"Creating node list...\")\n",
    "    nid = 0\n",
    "    node_list = {}\n",
    "\n",
    "    for keys, _ in edgelist.items():\n",
    "        for key in keys:\n",
    "            if key not in node_list.keys():\n",
    "                node_list[key] = nid\n",
    "                nid += 1\n",
    "    return node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nodes_edges(source_filename, dest_filename, cols, collapse_search, use_delooped_journeys,\n",
    "                        drop_incorrect_occ,\n",
    "                        with_attribute):\n",
    "    \"\"\"\n",
    "    Read processed_journey dataframe file, preprocess, compute node/edge lists, write contents of lists to file.\n",
    "    :param collapse_search:\n",
    "    :param with_attribute:\n",
    "    :param drop_incorrect_occ:\n",
    "    :param use_delooped_journeys:\n",
    "    :param source_filename: dataframe to be loaded\n",
    "    :param dest_filename: filename prefix for node and edge files\n",
    "    \"\"\"\n",
    "    df = read_file(source_filename, cols, collapse_search, use_delooped_journeys, drop_incorrect_occ, with_attribute)\n",
    "    edges = edgelist_from_subpaths(df, use_delooped_journeys)\n",
    "    node_list = nodes_from_edgelist(edges)\n",
    "\n",
    "    print(list(node_list.items())[0:10])\n",
    "\n",
    "    default_edge_header = \"Source_node\\tSource_id\\tDestination_node\\tDestination_id\\tWeight\\n\"\n",
    "    default_node_header = \"Node\\tNode_id\\n\"\n",
    "    node_attr = None\n",
    "\n",
    "    logger.info(\"Number of nodes: {} Number of edges: {}\".format(len(node_list), len(edges)))\n",
    "    logger.info(\"Writing edge list to file...\")\n",
    "\n",
    "    edge_writer(dest_filename + \"_edges.csv.gz\", default_edge_header, edges, node_list, node_attr)\n",
    "    node_writer(dest_filename + \"_nodes.csv.gz\", default_node_header, node_list, node_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_writer(filename, header, node_id, node_attr):\n",
    "    with gzip.open(filename, \"w\") as file:\n",
    "        print(filename)\n",
    "        file.write(header.encode())\n",
    "        for node, nid in node_id.items():\n",
    "            file.write(\"{}\\t{}\".format(node, nid).encode())\n",
    "            if node_attr is not None:\n",
    "                file.write(\"\\t{}\".format(node_attr[node]).encode())\n",
    "            file.write(\"\\n\".encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_writer(filename, header, edges, node_id, node_attr):\n",
    "    with gzip.open(filename, \"w\") as file:\n",
    "        print(filename)\n",
    "        file.write(header.encode())\n",
    "        for key, value in edges.items():\n",
    "            file.write(\"{}\\t{}\\t{}\\t{}\\t{}\".format(key[0], node_id[key[0]], key[1], node_id[key[1]], value).encode())\n",
    "            if node_attr is not None:\n",
    "                file.write(\"\\t{}\\t{}\".format(node_attr[key[0]], node_attr[key[1]]).encode())\n",
    "            file.write(\"\\n\".encode())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
